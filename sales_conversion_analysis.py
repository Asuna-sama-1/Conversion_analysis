# -*- coding: utf-8 -*-
"""Sales Conversion Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ULio4Y3ZZd3HXjGpiJg1DnujKK1lOuGM

# Conversion rate VS. Time
"""

pip install pandasql

pip install plotly

import os

import pandas as pd
import numpy as np 
from datetime import timedelta
import datetime
import matplotlib
import matplotlib.pyplot as plt
import seaborn as sns
import pandasql as ps
import plotly.express as px

"""## Load Data"""

data = pd.read_csv('cover_test_dataset.csv')
data

# Change data type 
data.iloc[:,1:]= data.iloc[:,1:].apply(pd.to_datetime)

data.describe(include='all',datetime_is_numeric =True)

"""## **Sales Funnel**

"""

# gettin not null values 
a= len(data)- data.isnull().sum()
a

from plotly import graph_objects as go
fig = go.Figure(go.Funnel(
    y = ['Users','Installation', 'Requested', 'Agent_responded', 'Customer_sold'],
    x = np.round(a[:],2),
    textposition = "inside",
    textinfo = "value+percent initial",
    opacity = 0.65)
    )

fig.show()

"""The funnel charts showed multiple conversion rates along the user journey. We can find the bottleneck, aka, where the customers are lost. 

As we can perfom a bottleneck-research from the graph above, we can see that 

- Installation 

    ↓   74.9%

- Requested 

    ↓  75.1%  

- Responded 

    ↓ 7.7% 
- Sold 

The data tells us that during the time period when the data was collected, 75% of users requested price, and our agent has reached out to 75% of them, but only 7.7% of them are successfully converted. 
- This means thE most of the customer lost happend AFTER customer submited their informatiom. There are some directions that we can look into:
   1. **how many customers comfirmed their interested in the price**?  
   we would record one more step (customers_interested); if most of the customer lost happend here, then the cause might be the price being too high or policy are not what they are looking for. 
   2. **how long is the wait time in the queue?**
   There is a high chance of losing costomer if we let them wait for too long. Reduce the wait time if this is the root cuase. 
   3. **agent's stratgy when speaking to customer.**

   How do we contact our customers? by phone? by email? 

   When are we contacting our customer? morning? night? weekdays?weenkends? 


There is also other areas we would look into:
- 
when customer installed the app, it's easy for them to navigate thru the app and knowing how to submit their information?

### How does wait time in the queue effect customer conversion?
"""

data1 = data.copy()

data1['wait_time'] = (data1['agent_responded_to_customer_at'] - data1['request_created_at']).dt.total_seconds()/60
data1.describe()

"""#### Distribution"""

sns.set()
sns.histplot(data=data1, x='wait_time', bins= 100)

q = """
    SELECT user_id, agent_responded_to_customer_at,request_created_at,wait_time
    FROM data1
    WHERE wait_time IS NULL OR wait_time <= 0 
    ORDER BY wait_time
"""
t=ps.sqldf(q, locals())

q = """
    SELECT SUM(CASE WHEN wait_time >0.00 THEN 1 ELSE 0 END) AS pos_time,
           SUM(CASE WHEN wait_time <0.00 THEN 1 ELSE 0 END) AS nag_time,
           SUM(CASE WHEN wait_time =0.00 THEN 1 ELSE 0 END) AS immi_res,
           SUM(CASE WHEN wait_time IS NULL THEN 1 ELSE 0 END ) AS na
    FROM data1
"""
w = ps.sqldf(q, locals())

pec=pd.DataFrame(w/len(data1)*100)
pec

plt.figure(figsize=(8,5))

ax = sns.barplot(data=w, capsize=.2)
plt.title('Distribution of respond time types', fontsize=15)

for p in ax.patches:
    percentage = '{:.3f}%'.format(100 * p.get_height()/len(data1))
    x = p.get_x() + p.get_width()
    y = p.get_height()
    ax.annotate(percentage, (x, y),ha='center')
plt.show()

"""There are 0.043% of the responds time are nagative, those might happend due to technique issue, thus we remove them. 

Our agent never responded to 43.89% of the customers because they’ve indicated they’re not interested in the insurance policy

Our immediately respond rate is only 1.9%. 

Majority of the responde rate is still positive. Now let's take a closer look at those time periods.

### Outliers

1. Remove nagetive values
2. Remove NA values
"""

q = """
    SELECT *
    FROM data1
    WHERE wait_time > 0.0 
"""
w = ps.sqldf(q, locals())

w['wait_time'].describe()

w['wait_time'].mode()

sns.boxplot(y=w['wait_time'])

"""our data is extreme skewed, Most data points are at the lower end. I will take the lower 25% as a threshold. """

q = """
    SELECT *
    FROM data1
    WHERE wait_time BETWEEN 0 AND 38
"""
sns.histplot(data=ps.sqldf(q, locals()) , x='wait_time')

q = """
    SELECT *
    FROM data1
    WHERE wait_time BETWEEN 0 AND  3 
"""
sns.histplot(data=ps.sqldf(q, locals()) , x='wait_time')

"""## Multiple Conversion Rate analysis

### 90 days aftrer installation
"""

df = data.copy()
df['offset_90'] = df['installation_at'] + pd.DateOffset(days=90)
df['offset_180'] = df['installation_at'] + pd.DateOffset(days=180)
df['offset_270'] = df['installation_at'] + pd.DateOffset(days=270)
df['offset_360'] = df['installation_at'] + pd.DateOffset(days=360)
df.head()

q = """
    SELECT *
    FROM 
    (SELECT 'Interval_1' AS Days, 
          COUNT(d2.customer_sold_at) AS Sold,
          COUNT(d2.agent_responded_to_customer_at) AS Responded,
          COUNT(d2.request_created_at) AS Requested
    FROM df d1 
    LEFT JOIN df d2 
      ON d1.user_id = d2.user_id 
    WHERE d1.customer_sold_at <= d2.offset_90 
UNION 
 SELECT 'Interval_2' AS Days, 
          COUNT(d2.customer_sold_at) AS Sold,
          COUNT(d2.agent_responded_to_customer_at) AS Responded,
          COUNT(d2.request_created_at) AS Requested
    FROM df d1 
    LEFT JOIN df d2 
      ON d1.user_id = d2.user_id 
    WHERE d1.customer_sold_at BETWEEN d2.offset_90 AND d2.offset_180 
UNION 
 SELECT 'Interval_3' AS Days, 
          COUNT(d2.customer_sold_at) AS Sold,
          COUNT(d2.agent_responded_to_customer_at) AS Responded,
          COUNT(d2.request_created_at) AS Requested
    FROM df d1 
    LEFT JOIN df d2 
      ON d1.user_id = d2.user_id 
    WHERE d1.customer_sold_at BETWEEN d2.offset_180 AND d2.offset_270 
UNION 
 SELECT 'Interval_4' AS Days, 
          COUNT(d2.customer_sold_at) AS Sold,
          COUNT(d2.agent_responded_to_customer_at) AS Responded,
          COUNT(d2.request_created_at) AS Requested
    FROM df d1 
    LEFT JOIN df d2 
      ON d1.user_id = d2.user_id 
    WHERE d1.customer_sold_at BETWEEN d2.offset_270 AND d2.offset_360) T
    ORDER BY Days
 """
t1=ps.sqldf(q, locals())

t1

sns.set(style='white')

#create stacked bar chart
t1.set_index('Days').plot(kind='bar', stacked=True)

#add overall title
plt.title('Conversion by 90d interval', fontsize=16)

#add axis titles
plt.xlabel('Days')
plt.ylabel('Number of conversion')

#rotate x-axis labels
plt.xticks(rotation=45)

l = t1['Sold']
for i,j in zip(range(1,5),l):
  print('Final Conversion Rate for',i,'th 90d interval is',np.round(j/132290*100,2),'%,','which is',np.round(j/5741*100,2),'% of total Conversed customers')

"""From above, we can see that majority of the conversions happended within the first 90d interval, which means we should try to investe most on engaging with our customer during this time frame.

### Waitime vs. Converation rate at each step
"""

df1 = data.copy()
df1 = df1.set_index('user_id')

df1.head()

df1['ins_req'] = (df1['request_created_at'] - df1['installation_at']).dt.total_seconds()/60
df1['req_res'] = (df1['agent_responded_to_customer_at'] - df1['request_created_at']).dt.total_seconds()/60
df1['res_sold'] = (df1['customer_sold_at'] - df1['agent_responded_to_customer_at']).dt.total_seconds()/60
df1.describe()

# creating wait time for each stage in mins

"""Filter out all nagetive values"""

q ="""
  SELECT *
  FROM df1
  WHERE ins_req < 0 or  req_res <0  or res_sold <0
"""
t = ps.sqldf(q, locals())
# nagative wait times

q ="""
  SELECT df1.*
  FROM df1
  LEFT JOIN t ON t.user_id = df1.user_id 
  WHERE t.user_id IS NULL
"""
dt2 = ps.sqldf(q, locals())
# dataframe without nagetive wait times

dt2.describe().mean()[-3:]

dt2_total= len(dt2)-dt2.isnull().sum()
dt2_total

conv_rate= [dt2_total.ins_req/dt2_total.installation_at, dt2_total.req_res/dt2_total.ins_req,dt2_total.res_sold/dt2_total.req_res]

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data = conv_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(x = dt2.columns[-3:]	, y =dt2.describe().mean()[-3:],alpha=0.5, ax=ax2).set(title='Average wait time vs. Multiple Conversion Rate')

"""## Conversion Rate Redefined

"""

d = data.copy()
d.iloc[:,1:]= d.iloc[:,1:].apply(pd.to_datetime)
d.dtypes

d['duration'] = (d['customer_sold_at'] - d['installation_at']).dt.days
d['period'] = (d['customer_sold_at'] - d['installation_at']).dt.days/90
d.describe()

"""#### Quarterly

remove all nagative values
"""

q = """
    SELECT SUM(CASE WHEN duration >0.00 THEN 1 ELSE 0 END) AS pos_time,
           SUM(CASE WHEN duration <0.00 THEN 1 ELSE 0 END) AS nag_time,
           SUM(CASE WHEN duration =0.00 THEN 1 ELSE 0 END) AS immi_res,
           SUM(CASE WHEN duration IS NULL THEN 1 ELSE 0 END ) AS na
    FROM d
"""
w = ps.sqldf(q, locals())

plt.figure(figsize=(8,5))

ax = sns.barplot(data=w, capsize=.2)
plt.title('Distribution of respond time types', fontsize=15)

for p in ax.patches:
    percentage = '{:.3f}%'.format(100 * p.get_height()/len(d))
    x = p.get_x() + p.get_width()
    y = p.get_height()
    ax.annotate(percentage, (x, y),ha='center')
plt.show()

q = """
    SELECT *
    FROM d WHERE duration >=0 OR duration is NULL 
"""
d = ps.sqldf(q, locals())

d.iloc[:,1:5]= d.iloc[:,1:5].apply(pd.to_datetime)
d.dtypes

q1 = min(d.installation_at)+ pd.DateOffset(days=90)
q2 = min(d.installation_at)+ pd.DateOffset(days=180)
q3 = min(d.installation_at)+ pd.DateOffset(days=270)
q4 = min(d.installation_at)+ pd.DateOffset(days=360)
q1_ = min(d.installation_at)+ pd.DateOffset(days=450)
print(q1,q2,q3,q4,q1_)

q = """
  SELECT 'Q1' As season,
        COUNT(customer_sold_at) AS sold,
        COUNT(installation_at) AS total
  FROM d
  WHERE installation_at<='2019-01-08' OR customer_sold_at<='2019-01-08' 
UNION 
  SELECT 'Q2' As season,
        COUNT(customer_sold_at) AS sold,
        COUNT(installation_at) AS total
  FROM d
  WHERE (installation_at BETWEEN '2019-01-09' AND '2019-04-08') OR (customer_sold_at BETWEEN '2019-01-09' AND '2019-04-08')  
 UNION 
  SELECT 'Q3' As season,
        COUNT(customer_sold_at) AS sold,
        COUNT(installation_at) AS total
  FROM d
  WHERE  (installation_at BETWEEN '2019-04-09' AND '2019-07-07') OR (customer_sold_at BETWEEN '2019-04-09' AND '2019-07-07') 
  UNION 
  SELECT 'Q4' As season,
        COUNT(customer_sold_at) AS sold,
        COUNT(installation_at) AS total
  FROM d
  WHERE (installation_at BETWEEN '2019-07-08' AND '2019-10-05')  OR (customer_sold_at BETWEEN '2019-07-08' AND '2019-10-05')  
UNION 
 SELECT 'Q5' As season,
        COUNT(customer_sold_at) AS sold,
        COUNT(installation_at) AS total
  FROM d
  WHERE installation_at >='2019-10-06'  OR  customer_sold_at >='2019-10-06'
   
"""
temp = ps.sqldf(q, locals())

temp['rate'] = np.round(temp['sold']/temp['total'],4)
temp

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data = temp['rate'], marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= temp, x = temp.season	, y = temp.sold,alpha=0.5, ax=ax2).set(title='Quarterly Conversion Rate')

"""### Feature Generation"""

df_time= d.copy().iloc[:,:-1]

df_time.dtypes

df_time['year'] = df_time['installation_at'].dt.strftime('%Y')
df_time['year_month'] = df_time['installation_at'].dt.strftime('%Y-%m')
df_time['day_week'] = df_time['installation_at'].dt.strftime('%a')
df_time['day'] = df_time['installation_at'].dt.strftime('%Y-%m-%d')
df_time['hour'] = df_time['installation_at'].dt.strftime('%H')

df_time.head()

"""### Monthly"""

q = """
  SELECT year_month, COUNT(user_id) AS total , Count(customer_sold_at) AS conversions,
  SUM( CASE WHEN customer_sold_at IS NULL THEN 1 ELSE 0 END) AS non_conversion
  FROM df_time
  GROUP BY year_month
  HAVING year IS NOT NULL
"""
monthly = ps.sqldf(q, locals())

monthly = monthly.set_index('year_month')

monthly

conversion_rate = monthly.conversions/ monthly.total
conversion_rate

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= monthly, x = monthly.index	, y = monthly.conversions,alpha=0.5, ax=ax2).set(title='Monthly Conversion Rate')

"""### Daily """

q = """
  SELECT day, COUNT(user_id) AS total , Count(customer_sold_at) AS conversions,
  SUM( CASE WHEN customer_sold_at IS NULL THEN 1 ELSE 0 END) AS non_conversion
  FROM df_time
  GROUP BY day
  HAVING year IS NOT NULL
"""
daily = ps.sqldf(q, locals())

daily

#daily = daily.set_index('day')
conversion_rate = daily.conversions/ daily.total

fig, ax1 = plt.subplots(figsize=(20,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= daily, x = daily.index	, y = daily.conversions,alpha=0.5, ax=ax2).set(title='Daily Conversion Rate')

"""### Weekly"""

q = """
  SELECT day_week, COUNT(user_id) AS total , Count(customer_sold_at) AS conversions,
  SUM( CASE WHEN customer_sold_at IS NULL THEN 1 ELSE 0 END) AS non_conversion
  FROM df_time
  GROUP BY day_week
  HAVING year IS NOT NULL
"""
weekly = ps.sqldf(q, locals())

weekly

weekly = weekly.set_index('day_week')

conversion_rate = weekly.conversions/ weekly.total

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= weekly, x = weekly.index	, y = weekly.conversions,alpha=0.5, ax=ax2).set(title='Weekly Conversion Rate')

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= weekly, x = weekly.index	, y = weekly.total,alpha=0.5, ax=ax2).set(title='Weekly Total Installs')

weekly['converation_rate'] =conversion_rate
weekly['cost_per_install'] = (weekly.total * 0.01) /weekly.converation_rate
weekly[['total','converation_rate','cost_per_install']]

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =weekly.cost_per_install, marker='o', sort = False, ax=ax1)

"""### Hourly """

q = """
  SELECT hour, COUNT(user_id) AS total , Count(customer_sold_at) AS conversions,
  SUM( CASE WHEN customer_sold_at IS NULL THEN 1 ELSE 0 END) AS non_conversion
  FROM df_time
  GROUP BY hour
  HAVING year IS NOT NULL
"""
hourly = ps.sqldf(q, locals())

hourly = hourly.set_index('hour')

conversion_rate = hourly.conversions/ hourly.total

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= hourly, x = hourly.index	, y = hourly.conversions,alpha=0.5, ax=ax2).set(title='Hourly Conversion Rate')

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =conversion_rate, marker='o', sort = False, ax=ax1)
ax2 = ax1.twinx()

sns.barplot(data= hourly, x = hourly.index	, y = hourly.total,alpha=0.5, ax=ax2).set(title='Hourly Total Installs')

hourly['converation_rate'] =conversion_rate
hourly['cost_per_install'] = (hourly.total * 0.01) /hourly.converation_rate
hourly[['total','converation_rate','cost_per_install']]

fig, ax1 = plt.subplots(figsize=(12,5))
sns.lineplot(data =hourly.cost_per_install, marker='o', sort = False, ax=ax1)

"""### 90 D interval"""

df_m= d.copy()
df_m.dtypes

df_m['offset_90'] = df_m['installation_at'] + pd.DateOffset(days=90)
#df_m['day']  = df_m['installation_at'].dt.strftime('%Y-%m-%d')
#df_m['offset_90']  = df_m['offset_90'].dt.strftime('%Y-%m-%d')
df_m['installation_at']= df_m['installation_at'].dt.date
df_m['offset_90']= df_m['offset_90'].dt.date

df_m.head()

df_time = df_m[['installation_at','customer_sold_at']].apply(pd.to_datetime)
df_time.dtypes

df_time.installation_at.iloc[1]

df_new = pd.DataFrame(columns=['total','sold'])
df_new

q = """SELECT DISTINCT installation_at, offset_90
      FROM df_m 
      WHERE installation_at IS NOT NULL
"""
interv = ps.sqldf(q,locals())

interv=interv.apply(pd.to_datetime)
interv.dtypes

total = []
sold =[]
for i in range(0,len(df_time)):
  user =0
  conv= 0
  for j in range(0,len(interv)):
    if df_time.installation_at.iloc[i] >= interv.iloc[j].values[0] and df_time.installation_at.iloc[i] <= interv.iloc[j].values[1]:
      user += 1
    if df_time.customer_sold_at.iloc[i] >= interv.iloc[j].values[0] and df_time.customer_sold_at.iloc[i] <= interv.iloc[j].values[1]:
      conv +=1
  total.append(user)
  sold.append(conv)

"""## Time Series Forcasting Model"""

daily['conversion_reate'] = daily.conversions/daily.total
daily =daily.reset_index()

daily.dtypes

"""### ARIMA"""

pip install statsmodels

from statsmodels.tsa.arima_model import ARIMA

daily.dtypes

rolling_mean = df.rolling(window = 90).mean()
rolling_std = df.rolling(window = 90).std()
plt.plot(df, color = 'blue', label = 'Original')
plt.plot(rolling_mean, color = 'red', label = 'Rolling Mean')
plt.plot(rolling_std, color = 'black', label = 'Rolling Std')
plt.legend(loc = 'best')
plt.title('Rolling Mean & Rolling Standard Deviation')
plt.show()

"""Plot the rolling mean and rolling standard deviation.he rolling mean and rolling standard deviation are almost flat with time. Therefore, we can conclude that the time series is  stationary.

"""

# split into train and test sets
X = daily.conversion_reate
size = int(len(X)*0.66)
train, test = X[0:size], X[size:len(X)]

# fit an ARIMA model
model = ARIMA(train, order=(5,1,1))
model_fit = model.fit()

print(model_fit.summary())

# line plot of residuals
residuals = pd.DataFrame(model_fit.resid)
residuals.plot()
plt.show()

# density plot of residuals
residuals.plot(kind='kde')
plt.show()

"""suggesting the errors are Gaussian, but may not be centered on zero."""

# summary stats of residuals
print(residuals.describe())

df= daily['conversion_reate']
df.index =daily['day']
df

from math import sqrt
from statsmodels.tsa.arima_model import ARIMA

from sklearn.metrics import mean_squared_error

X = daily.conversion_reate
size = int(len(X)*0.66)
train, test = X[0:size], X[size:len(X)]

type(test)

history = [x for x in train]
predictions = list()

# walk-forward validation
for t in range(len(test)):
	model = ARIMA(history, order=(5,1,0))
	model_fit = model.fit()
	output = model_fit.forecast()
	yhat = output[0]
	predictions.append(yhat)
	obs = test[t]
	history.append(obs)
	#print('predicted=%f, expected=%f' % (yhat, obs))
# evaluate forecasts
rmse = sqrt(mean_squared_error(test, predictions))
print('Test RMSE: %.3f's % rmse)

# plot forecasts against actual outcomes
plt.plot(test.values)
plt.plot(predictions, color='red')
plt.show()

"""The model could use further tuning of the p, d, and maybe even the q parameters.

### Prophet
"""

from fbprophet import Prophet

daily['day']= pd.to_datetime(daily['day'])

"""Input Features x and y """

x= daily.iloc[:,:-1]
y= daily.iloc[:,-1:]

"""creating dataset for prophet"""

train_dataset= pd.DataFrame()
train_dataset['ds'] = pd.to_datetime(x["day"])
train_dataset['y']=y
train_dataset.tail(2)

"""Creating and fitting the Prophet model with default values"""

m = Prophet()
m.fit(train_dataset)

"""Predicting the values for the future"""

future= m.make_future_dataframe(periods=360)
future.tail(2)

"""Total number of rows in original dataset was 723 and we see that the future data frame that we created for prediction contains historical dates as well as additional 360 dates."""

forecast = m.predict(future)
forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()

fig1 = m.plot(forecast)

fig2 = m.plot_components(forecast)

#plot the trend and seasonality, components of the forecast.

pip install prophet

from fbprophet import Prophet
from prophet.plot import plot_plotly, plot_components_plotly

plot_plotly(m, forecast)

"""### Evaluation"""

from prophet.diagnostics import cross_validation

df_cv = cross_validation(m, initial='90 days', period='90 days', horizon = '90 days')

df_cv

from prophet.diagnostics import performance_metrics
df_p = performance_metrics(df_cv)
df_p.head()

from prophet.plot import plot_cross_validation_metric

fig = plot_cross_validation_metric(df_cv, metric='rmse')

